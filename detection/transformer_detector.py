# =============================================================================
# detection/transformer_detector.py - Transformerå¼‚å¸¸æ£€æµ‹å™¨
# =============================================================================
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
import math


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç ï¼šè¡¨ç¤ºç½‘ç»œå±‚æ¬¡å…³ç³»"""
    def __init__(self, d_model, max_len=10):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:x.size(0), :]


class ActivationTransformer(nn.Module):
    """åŸºäºæ¿€æ´»åºåˆ—çš„Transformerå¼‚å¸¸æ£€æµ‹å™¨"""
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.feature_dim = config.transformer_dim
        self.num_layers = len(config.selected_layers)

        # ç‰¹å¾æŠ•å½±å±‚ï¼šå°†åŸå§‹æ¿€æ´»ç‰¹å¾æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦
        self.feature_projection = nn.Linear(config.raw_feature_dim, self.feature_dim)

        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(self.feature_dim, self.num_layers)

        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.feature_dim,
            nhead=config.transformer_heads,
            dim_feedforward=config.transformer_ff_dim,
            dropout=config.transformer_dropout,
            batch_first=True,
            activation='gelu'
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=config.transformer_layers
        )

        # å¼‚å¸¸æ£€æµ‹å¤´
        self.anomaly_detector = nn.Sequential(
            nn.Linear(self.feature_dim, config.transformer_ff_dim),
            nn.GELU(),
            nn.Dropout(config.transformer_dropout),
            nn.Linear(config.transformer_ff_dim, config.transformer_ff_dim // 2),
            nn.GELU(),
            nn.Dropout(config.transformer_dropout),
            nn.Linear(config.transformer_ff_dim // 2, 1),
            nn.Sigmoid()
        )

        # é‡æ„å¤´ï¼ˆç”¨äºè‡ªç›‘ç£é¢„è®­ç»ƒï¼‰
        self.reconstruction_head = nn.Sequential(
            nn.Linear(self.feature_dim, config.transformer_ff_dim),
            nn.GELU(),
            nn.Linear(config.transformer_ff_dim, config.raw_feature_dim)
        )

        # Layer norm
        self.layer_norm = nn.LayerNorm(self.feature_dim)

    def forward(self, x, return_attention=False):
        """
        Args:
            x: [batch_size, num_layers, raw_feature_dim] æ¿€æ´»åºåˆ—
        Returns:
            anomaly_scores: [batch_size, 1] å¼‚å¸¸åˆ†æ•°
        """
        batch_size, seq_len, _ = x.shape

        # ç‰¹å¾æŠ•å½±
        x = self.feature_projection(x)  # [batch, seq_len, feature_dim]
        x = self.layer_norm(x)

        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x.transpose(0, 1)  # [seq_len, batch, feature_dim]
        x = self.pos_encoding(x)
        x = x.transpose(0, 1)  # [batch, seq_len, feature_dim]

        # Transformerç¼–ç 
        if return_attention:
            # å¦‚æœéœ€è¦è¿”å›æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºå¯è§†åŒ–åˆ†æï¼‰
            encoded, attention_weights = self.transformer(x, return_attention=True)
        else:
            encoded = self.transformer(x)  # [batch, seq_len, feature_dim]

        # å…¨å±€ç‰¹å¾èšåˆï¼ˆå–å¹³å‡ï¼‰
        global_features = torch.mean(encoded, dim=1)  # [batch, feature_dim]

        # å¼‚å¸¸æ£€æµ‹
        anomaly_scores = self.anomaly_detector(global_features)  # [batch, 1]

        if return_attention:
            return anomaly_scores, attention_weights
        return anomaly_scores

    def reconstruct(self, x, mask_ratio=0.15):
        """è‡ªç›‘ç£é‡æ„ä»»åŠ¡"""
        batch_size, seq_len, _ = x.shape

        # éšæœºæ©ç 
        mask = torch.rand(batch_size, seq_len) < mask_ratio
        x_masked = x.clone()
        x_masked[mask] = 0  # æ©ç›–éƒ¨åˆ†ç‰¹å¾

        # ç¼–ç 
        x_proj = self.feature_projection(x_masked)
        x_proj = self.layer_norm(x_proj)
        x_proj = x_proj.transpose(0, 1)
        x_proj = self.pos_encoding(x_proj)
        x_proj = x_proj.transpose(0, 1)

        encoded = self.transformer(x_proj)

        # é‡æ„
        reconstructed = self.reconstruction_head(encoded)

        return reconstructed, mask


class UnsupervisedTransformerDetector:
    """æ— ç›‘ç£Transformeræ£€æµ‹å™¨è®­ç»ƒå’Œæ¨ç†"""
    def __init__(self, config):
        self.config = config
        self.device = config.device

        # åˆå§‹åŒ–æ¨¡å‹
        self.model = ActivationTransformer(config).to(self.device)

        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=config.transformer_lr,
            weight_decay=config.transformer_weight_decay
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config.transformer_epochs
        )

        # æŸå¤±å‡½æ•°
        self.reconstruction_loss = nn.MSELoss()

        # å¼‚å¸¸æ£€æµ‹é˜ˆå€¼ï¼ˆè®­ç»ƒåç¡®å®šï¼‰
        self.anomaly_threshold = None

    def pretrain_reconstruction(self, activation_sequences):
        """è‡ªç›‘ç£é¢„è®­ç»ƒï¼šé‡æ„ä»»åŠ¡"""
        print("ğŸ¯ å¼€å§‹è‡ªç›‘ç£é¢„è®­ç»ƒ...")

        # å‡†å¤‡æ•°æ®ï¼ˆåªä½¿ç”¨è‰¯æ€§æ ·æœ¬è¿›è¡Œé¢„è®­ç»ƒï¼‰
        X_tensor = torch.FloatTensor(activation_sequences).to(self.device)
        dataset = TensorDataset(X_tensor)
        dataloader = DataLoader(
            dataset,
            batch_size=self.config.transformer_batch_size,
            shuffle=True
        )

        self.model.train()

        for epoch in range(self.config.pretrain_epochs):
            epoch_loss = 0

            for batch_x, in dataloader:
                self.optimizer.zero_grad()

                # é‡æ„ä»»åŠ¡
                reconstructed, mask = self.model.reconstruct(
                    batch_x, self.config.mask_ratio
                )

                # åªè®¡ç®—è¢«æ©ç›–ä½ç½®çš„é‡æ„æŸå¤±
                loss = self.reconstruction_loss(
                    reconstructed[mask],
                    batch_x[mask]
                )

                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                self.optimizer.step()

                epoch_loss += loss.item()

            if (epoch + 1) % 10 == 0:
                avg_loss = epoch_loss / len(dataloader)
                print(f"é¢„è®­ç»ƒ Epoch {epoch + 1}: é‡æ„æŸå¤± = {avg_loss:.6f}")

        print("âœ… é¢„è®­ç»ƒå®Œæˆ!")

    def train_anomaly_detector(self, benign_sequences, malicious_sequences):
        """è®­ç»ƒå¼‚å¸¸æ£€æµ‹å™¨ï¼ˆä½¿ç”¨å°‘é‡æ ‡æ³¨æ•°æ®æˆ–å®Œå…¨æ— ç›‘ç£ï¼‰"""
        print("ğŸ›¡ï¸ è®­ç»ƒå¼‚å¸¸æ£€æµ‹å™¨...")

        if self.config.unsupervised_only:
            # å®Œå…¨æ— ç›‘ç£ï¼šåªä½¿ç”¨è‰¯æ€§æ ·æœ¬å­¦ä¹ æ­£å¸¸æ¨¡å¼
            return self._train_fully_unsupervised(benign_sequences)
        else:
            # åŠç›‘ç£ï¼šä½¿ç”¨å°‘é‡æ ‡æ³¨æ•°æ®
            return self._train_semi_supervised(benign_sequences, malicious_sequences)

    def _train_fully_unsupervised(self, benign_sequences):
        """å®Œå…¨æ— ç›‘ç£è®­ç»ƒ"""
        print("ğŸ“Š ä½¿ç”¨å®Œå…¨æ— ç›‘ç£æ–¹æ³•...")

        # åªä½¿ç”¨è‰¯æ€§æ ·æœ¬
        X_tensor = torch.FloatTensor(benign_sequences).to(self.device)

        # è®¡ç®—æ­£å¸¸æ ·æœ¬çš„å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒ
        self.model.eval()
        with torch.no_grad():
            anomaly_scores = []
            for i in range(0, len(X_tensor), self.config.transformer_batch_size):
                batch = X_tensor[i:i+self.config.transformer_batch_size]
                scores = self.model(batch)
                anomaly_scores.append(scores.cpu().numpy())

            anomaly_scores = np.concatenate(anomaly_scores, axis=0)

        # è®¾ç½®é˜ˆå€¼ï¼šæ­£å¸¸æ ·æœ¬95%åˆ†ä½æ•°
        self.anomaly_threshold = np.percentile(anomaly_scores, 95)

        print(f"å¼‚å¸¸æ£€æµ‹é˜ˆå€¼è®¾å®šä¸º: {self.anomaly_threshold:.4f}")

        # è¿”å›è™šæ‹ŸæŒ‡æ ‡ï¼ˆå› ä¸ºæ²¡æœ‰çœŸå®æ ‡ç­¾ï¼‰
        return {
            'threshold': self.anomaly_threshold,
            'benign_scores_mean': np.mean(anomaly_scores),
            'benign_scores_std': np.std(anomaly_scores)
        }

    def _train_semi_supervised(self, benign_sequences, malicious_sequences):
        """åŠç›‘ç£è®­ç»ƒï¼ˆä½¿ç”¨å°‘é‡æ ‡æ³¨æ•°æ®ä¼˜åŒ–é˜ˆå€¼ï¼‰"""
        print("ğŸ“Š ä½¿ç”¨åŠç›‘ç£æ–¹æ³•...")

        # åˆå¹¶æ•°æ®
        X = np.concatenate([benign_sequences, malicious_sequences], axis=0)
        y = np.concatenate([
            np.zeros(len(benign_sequences)),
            np.ones(len(malicious_sequences))
        ])

        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )

        # è½¬æ¢ä¸ºå¼ é‡
        X_train_tensor = torch.FloatTensor(X_train).to(self.device)
        X_test_tensor = torch.FloatTensor(X_test).to(self.device)

        # è·å–å¼‚å¸¸åˆ†æ•°
        self.model.eval()
        with torch.no_grad():
            train_scores = self.model(X_train_tensor).cpu().numpy()
            test_scores = self.model(X_test_tensor).cpu().numpy()

        # å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
        best_threshold = self._find_optimal_threshold(train_scores, y_train)
        self.anomaly_threshold = best_threshold

        # è¯„ä¼°æ€§èƒ½
        test_predictions = (test_scores > best_threshold).astype(int)
        metrics = {
            'accuracy': accuracy_score(y_test, test_predictions),
            'precision': precision_score(y_test, test_predictions),
            'recall': recall_score(y_test, test_predictions),
            'f1_score': f1_score(y_test, test_predictions),
            'threshold': best_threshold
        }

        print(f"æœ€ä¼˜é˜ˆå€¼: {best_threshold:.4f}")
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
        print(f"æµ‹è¯•F1åˆ†æ•°: {metrics['f1_score']:.4f}")

        return metrics

    def _find_optimal_threshold(self, scores, labels):
        """å¯»æ‰¾æœ€ä¼˜å¼‚å¸¸æ£€æµ‹é˜ˆå€¼"""
        thresholds = np.linspace(scores.min(), scores.max(), 100)
        best_f1 = 0
        best_threshold = thresholds[0]

        for threshold in thresholds:
            predictions = (scores > threshold).astype(int)
            f1 = f1_score(labels, predictions)

            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold

        return best_threshold

    def detect_anomalies(self, activation_sequences):
        """æ£€æµ‹å¼‚å¸¸ï¼ˆæ¨ç†é˜¶æ®µï¼‰"""
        self.model.eval()

        X_tensor = torch.FloatTensor(activation_sequences).to(self.device)

        with torch.no_grad():
            anomaly_scores = self.model(X_tensor).cpu().numpy()

        # äºŒå€¼åŒ–ç»“æœ
        predictions = (anomaly_scores > self.anomaly_threshold).astype(int)

        return {
            'anomaly_scores': anomaly_scores,
            'predictions': predictions,
            'threshold': self.anomaly_threshold
        }

    def get_attention_analysis(self, activation_sequences):
        """è·å–æ³¨æ„åŠ›æƒé‡åˆ†æï¼ˆå¯è§£é‡Šæ€§ï¼‰"""
        self.model.eval()

        X_tensor = torch.FloatTensor(activation_sequences).to(self.device)

        with torch.no_grad():
            anomaly_scores, attention_weights = self.model(
                X_tensor, return_attention=True
            )

        return {
            'anomaly_scores': anomaly_scores.cpu().numpy(),
            'attention_weights': attention_weights.cpu().numpy(),
            'layer_names': self.config.selected_layers
        }

    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'threshold': self.anomaly_threshold,
            'config': self.config
        }, path)

    def load_model(self, path):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.anomaly_threshold = checkpoint['threshold']