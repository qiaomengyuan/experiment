# =============================================================================
# persistence/feature_extractor.py - Transformerç‰¹å¾æå–å™¨
# =============================================================================
import torch
import numpy as np
from torch.nn import functional as F


class TransformerFeatureExtractor:
    """ä¸ºTransformerå‡†å¤‡æ¿€æ´»åºåˆ—ç‰¹å¾"""
    def __init__(self, config):
        self.config = config
        self.device = config.device

    def extract_activation_sequences(self, models, dataloader):
        """ä»æ¨¡å‹åˆ—è¡¨ä¸­æå–æ¿€æ´»åºåˆ—"""
        print("ğŸ” æå–æ¿€æ´»åºåˆ—ç‰¹å¾...")

        all_sequences = []

        for i, model in enumerate(models):
            model = model.to(self.device)
            model.eval()

            sequence = self._extract_single_model_sequence(model, dataloader)
            all_sequences.append(sequence)

            if (i + 1) % 10 == 0:
                print(f"å·²å¤„ç† {i + 1}/{len(models)} ä¸ªæ¨¡å‹")

        sequences_array = np.array(all_sequences)
        print(f"âœ… æå–å®Œæˆ! åºåˆ—å½¢çŠ¶: {sequences_array.shape}")

        return sequences_array

    def _extract_single_model_sequence(self, model, dataloader):
        """ä»å•ä¸ªæ¨¡å‹æå–æ¿€æ´»åºåˆ—"""
        layer_features = []

        # ä¸ºæ¯ä¸ªç›®æ ‡å±‚æå–ç‰¹å¾
        for layer_name in self.config.selected_layers:
            layer_feature = self._extract_layer_feature(model, dataloader, layer_name)
            layer_features.append(layer_feature)

        # ç»„ç»‡æˆåºåˆ—æ ¼å¼: [num_layers, feature_dim]
        sequence = np.stack(layer_features, axis=0)

        return sequence

    def _extract_layer_feature(self, model, dataloader, layer_name):
        """ä»æŒ‡å®šå±‚æå–ç‰¹å¾å‘é‡"""
        all_activations = []
        sample_count = 0

        with torch.no_grad():
            for batch_idx, (data, _) in enumerate(dataloader):
                if sample_count >= self.config.num_samples:
                    break

                data = data.to(self.device)
                _ = model(data)  # å‰å‘ä¼ æ’­è§¦å‘hook

                # è·å–æ¿€æ´»å€¼
                activations = model.get_activations()

                if layer_name in activations:
                    activation = activations[layer_name]

                    # å¤„ç†ä¸åŒå±‚çš„æ¿€æ´»å½¢çŠ¶
                    if len(activation.shape) == 4:  # Convå±‚: [batch, channel, height, width]
                        # å…¨å±€å¹³å‡æ± åŒ–
                        pooled = F.adaptive_avg_pool2d(activation, (1, 1))
                        pooled = pooled.view(pooled.size(0), -1)  # [batch, channel]
                    elif len(activation.shape) == 2:  # FCå±‚: [batch, features]
                        pooled = activation
                    else:
                        # å…¶ä»–æƒ…å†µï¼šå±•å¹³åå–å¹³å‡
                        pooled = activation.view(activation.size(0), -1)

                    all_activations.append(pooled.cpu())
                    sample_count += data.size(0)

        if all_activations:
            # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
            concatenated = torch.cat(all_activations, dim=0)

            # è®¡ç®—ç»Ÿè®¡ç‰¹å¾å‘é‡
            feature_vector = self._compute_comprehensive_features(concatenated)
        else:
            # å¦‚æœæ²¡æœ‰æ¿€æ´»å€¼ï¼Œè¿”å›é›¶å‘é‡
            feature_vector = np.zeros(self.config.raw_feature_dim)

        return feature_vector

    def _compute_comprehensive_features(self, activations):
        """è®¡ç®—å…¨é¢çš„ç»Ÿè®¡ç‰¹å¾"""
        if isinstance(activations, torch.Tensor):
            activations = activations.numpy()

        # å¤„ç†å¼‚å¸¸å€¼
        activations = np.nan_to_num(activations, nan=0.0, posinf=1e6, neginf=-1e6)

        if activations.shape[0] == 0:
            return np.zeros(self.config.raw_feature_dim)

        features = []

        try:
            # 1. åŸºç¡€ç»Ÿè®¡é‡ (8ä¸ªç‰¹å¾)
            features.extend([
                np.mean(activations),                    # å…¨å±€å‡å€¼
                np.std(activations),                     # å…¨å±€æ ‡å‡†å·®
                np.max(activations),                     # å…¨å±€æœ€å¤§å€¼
                np.min(activations),                     # å…¨å±€æœ€å°å€¼
                np.median(activations),                  # å…¨å±€ä¸­ä½æ•°
                np.var(activations),                     # å…¨å±€æ–¹å·®
                np.ptp(activations),                     # å³°å€¼åˆ°å³°å€¼
                np.mean(np.abs(activations))             # å¹³å‡ç»å¯¹å€¼
            ])

            # 2. åˆ†ä½æ•°ç‰¹å¾ (7ä¸ªç‰¹å¾)
            percentiles = [5, 10, 25, 50, 75, 90, 95]
            for p in percentiles:
                features.append(np.percentile(activations, p))

            # 3. åˆ†å¸ƒå½¢çŠ¶ç‰¹å¾ (4ä¸ªç‰¹å¾)
            from scipy import stats
            features.extend([
                stats.skew(activations.flatten()),       # ååº¦
                stats.kurtosis(activations.flatten()),   # å³°åº¦
                stats.entropy(np.histogram(activations, bins=50)[0] + 1e-10),  # ç†µ
                np.sqrt(np.mean((activations - np.mean(activations))**2))     # RMS
            ])

            # 4. é€šé“é—´ç‰¹å¾ (å¦‚æœæ˜¯å¤šé€šé“)
            if len(activations.shape) > 1 and activations.shape[1] > 1:
                # é€šé“é—´ç›¸å…³æ€§ç‰¹å¾ (5ä¸ªç‰¹å¾)
                channel_means = np.mean(activations, axis=0)
                channel_stds = np.std(activations, axis=0)

                features.extend([
                    np.mean(channel_means),              # é€šé“å‡å€¼çš„å‡å€¼
                    np.std(channel_means),               # é€šé“å‡å€¼çš„æ ‡å‡†å·®
                    np.mean(channel_stds),               # é€šé“æ ‡å‡†å·®çš„å‡å€¼
                    np.std(channel_stds),                # é€šé“æ ‡å‡†å·®çš„æ ‡å‡†å·®
                    np.mean(np.corrcoef(activations.T)) if activations.shape[1] > 1 else 0  # å¹³å‡ç›¸å…³ç³»æ•°
                ])

                # ä¸»æˆåˆ†åˆ†æç‰¹å¾ (3ä¸ªç‰¹å¾)
                try:
                    from sklearn.decomposition import PCA
                    pca = PCA(n_components=min(3, activations.shape[1]))
                    pca.fit(activations)
                    features.extend(pca.explained_variance_ratio_.tolist())
                    # è¡¥é½åˆ°3ä¸ªç‰¹å¾
                    while len(features) < 27:  # 8+7+4+5+3=27
                        features.append(0.0)
                except:
                    features.extend([0.0, 0.0, 0.0])
            else:
                # å•é€šé“æƒ…å†µï¼Œç”¨é›¶å¡«å……
                features.extend([0.0] * 8)  # 5+3=8ä¸ªé›¶ç‰¹å¾

            # 5. æ¿€æ´»æ¨¡å¼ç‰¹å¾ (5ä¸ªç‰¹å¾)
            features.extend([
                np.sum(activations > 0) / activations.size,  # æ¿€æ´»ç‡ï¼ˆReLUåï¼‰
                np.sum(activations > np.mean(activations)) / activations.size,  # é«˜äºå‡å€¼çš„æ¯”ä¾‹
                np.sum(np.abs(activations) < 1e-6) / activations.size,  # æ¥è¿‘é›¶çš„æ¯”ä¾‹
                np.mean(activations[activations > 0]) if np.any(activations > 0) else 0,  # æ­£æ¿€æ´»å‡å€¼
                np.mean(activations[activations < 0]) if np.any(activations < 0) else 0   # è´Ÿæ¿€æ´»å‡å€¼
            ])

        except Exception as e:
            print(f"ç‰¹å¾è®¡ç®—é”™è¯¯: {e}")
            # å‘ç”Ÿé”™è¯¯æ—¶è¿”å›åŸºç¡€ç‰¹å¾
            features = [
                           np.mean(activations), np.std(activations), np.max(activations), np.min(activations)
                       ] + [0.0] * (self.config.raw_feature_dim - 4)

        # ç¡®ä¿ç‰¹å¾æ•°é‡æ­£ç¡®
        if len(features) > self.config.raw_feature_dim:
            features = features[:self.config.raw_feature_dim]
        elif len(features) < self.config.raw_feature_dim:
            features.extend([0.0] * (self.config.raw_feature_dim - len(features)))

        # æœ€ç»ˆæ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        features = np.array(features)
        features = np.nan_to_num(features, nan=0.0, posinf=1e6, neginf=-1e6)

        # ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆå¯é€‰ï¼‰
        if self.config.normalize_features:
            features = self._normalize_features(features)

        return features

    def _normalize_features(self, features):
        """ç‰¹å¾æ ‡å‡†åŒ–"""
        # é¿å…é™¤é›¶
        std = np.std(features)
        if std > 1e-8:
            features = (features - np.mean(features)) / std

        # è£å‰ªæå€¼
        features = np.clip(features, -5, 5)

        return features

    def generate_sequences_for_models(self, benign_models, malicious_models, dataloader):
        """ä¸ºè‰¯æ€§å’Œæ¶æ„æ¨¡å‹ç”Ÿæˆæ¿€æ´»åºåˆ—"""
        print("ğŸ”„ ç”Ÿæˆæ¨¡å‹æ¿€æ´»åºåˆ—...")

        # æå–è‰¯æ€§æ¨¡å‹åºåˆ—
        print("å¤„ç†è‰¯æ€§æ¨¡å‹...")
        benign_sequences = self.extract_activation_sequences(benign_models, dataloader)

        # æå–æ¶æ„æ¨¡å‹åºåˆ—
        print("å¤„ç†æ¶æ„æ¨¡å‹...")
        malicious_sequences = self.extract_activation_sequences(malicious_models, dataloader)

        print(f"âœ… åºåˆ—ç”Ÿæˆå®Œæˆ!")
        print(f"è‰¯æ€§åºåˆ—å½¢çŠ¶: {benign_sequences.shape}")
        print(f"æ¶æ„åºåˆ—å½¢çŠ¶: {malicious_sequences.shape}")

        return benign_sequences, malicious_sequences

    def extract_sequence_for_single_model(self, model, dataloader):
        """ä¸ºå•ä¸ªæ¨¡å‹æå–åºåˆ—ï¼ˆç”¨äºåœ¨çº¿æ£€æµ‹ï¼‰"""
        sequence = self._extract_single_model_sequence(model, dataloader)
        return sequence.reshape(1, *sequence.shape)  # æ·»åŠ batchç»´åº¦


class SequenceDataGenerator:
    """æ¿€æ´»åºåˆ—æ•°æ®ç”Ÿæˆå™¨"""
    def __init__(self, extractor):
        self.extractor = extractor

    def generate_from_server(self, server, dataloader):
        """ä»è”é‚¦å­¦ä¹ æœåŠ¡å™¨ç”Ÿæˆåºåˆ—æ•°æ®"""
        print("ğŸ“¦ ä»è”é‚¦å­¦ä¹ æœåŠ¡å™¨æ”¶é›†æ¨¡å‹...")

        benign_models, malicious_models = server.get_models_for_analysis()

        # å¹³è¡¡æ ·æœ¬æ•°é‡
        min_samples = min(len(benign_models), len(malicious_models))
        if min_samples < 10:
            print(f"âš ï¸ æ ·æœ¬æ•°é‡è¿‡å°‘: è‰¯æ€§={len(benign_models)}, æ¶æ„={len(malicious_models)}")

            # å¦‚æœæ ·æœ¬å¤ªå°‘ï¼Œé‡å¤æ ·æœ¬
            while len(benign_models) < 20:
                additional_samples = min(len(benign_models), 20 - len(benign_models))
                benign_models.extend(benign_models[:additional_samples])
            while len(malicious_models) < 20:
                additional_samples = min(len(malicious_models), 20 - len(malicious_models))
                malicious_models.extend(malicious_models[:additional_samples])

        # é™åˆ¶æ ·æœ¬æ•°é‡
        benign_models = benign_models[:30]
        malicious_models = malicious_models[:30]

        print(f"ä½¿ç”¨æ¨¡å‹: {len(benign_models)}ä¸ªè‰¯æ€§, {len(malicious_models)}ä¸ªæ¶æ„")

        # ç”Ÿæˆåºåˆ—
        benign_sequences, malicious_sequences = self.extractor.generate_sequences_for_models(
            benign_models, malicious_models, dataloader
        )

        return benign_sequences, malicious_sequences